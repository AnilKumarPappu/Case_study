{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Priyank\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %%time\n",
    "\n",
    "# # Third-party imports\n",
    "# import os.path as op\n",
    "# import pandas as pd\n",
    "# import great_expectations as ge\n",
    "\n",
    "# # Project imports\n",
    "# from ta_lib.core.api import display_as_tabs, initialize_environment\n",
    "\n",
    "# # Initialization\n",
    "# initialize_environment(debug=False, hide_warnings=True)\n",
    "# # abcdef\n",
    "# # Data\n",
    "\n",
    "# from ta_lib.core.api import create_context, list_datasets, load_dataset\n",
    "\n",
    "# config_path = op.join('conf', 'config.yml')\n",
    "# context = create_context(config_path)\n",
    "# list_datasets(context)\n",
    "\n",
    "# # load datasets\n",
    "# g_search = load_dataset(context, 'raw/google_search_data')\n",
    "# product = load_dataset(context, 'raw/product_manufacturer_list')\n",
    "# sales = load_dataset(context, 'raw/sales_data')\n",
    "# media = load_dataset(context, 'raw/social_media_data')\n",
    "# themes = load_dataset(context, 'raw/theme_list')\n",
    "# theme_product = load_dataset(context, 'raw/theme_product_list')\n",
    "# # Exploratory Analysis\n",
    "\n",
    "\n",
    "# # Import the eda API\n",
    "# import ta_lib.eda.api as eda\n",
    "# ## Variable summary\n",
    "# # variable summmary \n",
    "# display_as_tabs([('google_search', g_search.shape), ('product_manufacturer_list', product.shape),\n",
    "#                  ('sales_data', sales.shape), ('social_media', media.shape),\n",
    "#                  ('theme_list', themes.shape), ('theme_product_list', theme_product.shape)])\n",
    "\n",
    "# sum1 = eda.get_variable_summary(g_search)\n",
    "# sum2 = eda.get_variable_summary(product)\n",
    "# sum3 = eda.get_variable_summary(sales)\n",
    "# sum4 = eda.get_variable_summary(media)\n",
    "# sum5 = eda.get_variable_summary(themes)\n",
    "# sum6 = eda.get_variable_summary(theme_product)\n",
    "# display_as_tabs([('google_search', sum1), ('product_manufacturer_list', sum2), ('sales_data', sum3), \n",
    "#                 ('social_media', sum4), ('theme_list', sum5), ('theme_product_list', sum6)])\n",
    "# # Null Values checking \n",
    "\n",
    "\n",
    "# display_as_tabs([('google_search', g_search.isna().sum()), ('product_manufacturer_list', product.isna().sum()),\n",
    "#                  ('sales_data', sales.isna().sum()), ('social_media', media.isna().sum()),\n",
    "#                  ('theme_list', themes.isna().sum()), ('theme_product_list', theme_product.isna().sum())])\n",
    "# In social media data set , Theme id got lot missing values \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns \n",
    "\n",
    "# sns.heatmap(media.isnull(),cmap='viridis')\n",
    "# plt.show()\n",
    "# merged_data_sales = pd.merge(sales,theme_product, left_on='product_id', right_on='PRODUCT_ID')\n",
    "# merged_data_sales = pd.merge(merged_data_sales,themes, left_on='CLAIM_ID', right_on='CLAIM_ID')\n",
    "\n",
    "# merged_data_sales.nunique()\n",
    "# # So there are 49 unique theme present in Sales data list and their names are shown below:\n",
    "# merged_data_sales['Claim Name'].unique()\n",
    "# merged_data_google = pd.merge(g_search,themes, left_on='Claim_ID', right_on='CLAIM_ID')\n",
    "# merged_data_google.nunique()\n",
    "# # So there are 160 unique theme name in google search List and they are named below : \n",
    "# merged_data_google['Claim Name'].unique()\n",
    "# merged_data_social = pd.merge(media,themes, left_on='Theme Id', right_on='CLAIM_ID')\n",
    "# merged_data_social.nunique()\n",
    "# # So there are 193 unique theme name in Social media List and they are named below : \n",
    "# merged_data_social['Claim Name'].unique()\n",
    "# # Now Finding Common theme lsit in all three datasets \n",
    "# common=set(merged_data_google['Claim Name']) & set(merged_data_sales['Claim Name']) & set(merged_data_social['Claim Name'])\n",
    "# common_list=list(common)\n",
    "\n",
    "# len(common_list)\n",
    "# # SO there are 30 common themes present in all three dataset and are named below : \n",
    "# common_list\n",
    "\n",
    "\n",
    "\n",
    "# <details>\n",
    "# 1. Datatypes : We have both numeric and other types. The bulk of them seem to be numeric. `Numeric` is defined to be one of [float|int|date] and the rest are categorized as `Others`. A column is assumed to have `date` values if it has the string `date` in the column name.\n",
    "\n",
    "# ## Merging\n",
    "\n",
    "# We can merge orders table with prod table on SKU. Let us check first-cut cardinality issues. \n",
    "\n",
    "# ### Expected data validation rules\n",
    "# 1. Quantity should be an integer\n",
    "# 2. Quantity * UnitCost = SellingCost\n",
    "# 3. Quantity * UnitPrice = SellingPrice\n",
    "# # verification_dict = {}\n",
    "# # orders_df = ge.from_pandas(orders_df)\n",
    "# #### Rule 1 verification\n",
    "# # verification_dict[\"rule_1_check\"] = orders_df.expect_column_values_to_be_of_type(\"Quantity\", \"int64\", mostly=None, \n",
    "# #                                              result_format=\"BASIC\", include_config=True).to_json_dict()\n",
    "\n",
    "# # if verification_dict[\"rule_1_check\"][\"success\"]:\n",
    "# #     print(\"Rule 1 passed\")\n",
    "# # else:\n",
    "# #     print(\"Rule 1 failed\")\n",
    "# #### Rule 2 verification\n",
    "# # orders_df[\"selling_cal\"] = orders_df[\"Quantity\"] * orders_df[\"UnitCost\"]\n",
    "# # orders_df.selling_cal = orders_df.selling_cal.round()\n",
    "# # orders_df[\"act_selling_round\"] = orders_df.SellingCost.round()\n",
    "# # verification_dict[\"rule_2_check\"] = orders_df.expect_column_pair_values_to_be_equal(\"selling_cal\", \"act_selling_round\", mostly=None, \n",
    "# #                                              result_format=\"BASIC\", include_config=True).to_json_dict()\n",
    "\n",
    "# # if verification_dict[\"rule_2_check\"][\"success\"]:\n",
    "# #     print(\"Rule 2 passed\")\n",
    "# # else:\n",
    "# #     print(\"Rule 2 failed\")\n",
    "# #### Rule 3 verification\n",
    "# # orders_df[\"selling_cal\"] = orders_df[\"Quantity\"] * orders_df[\"UnitPrice\"]\n",
    "# # orders_df.selling_cal = orders_df.selling_cal.round()\n",
    "# # orders_df[\"act_selling_round\"] = orders_df.SellingPrice.round()\n",
    "# # verification_dict[\"rule_3_check\"] = orders_df.expect_column_pair_values_to_be_equal(\"selling_cal\", \"act_selling_round\", mostly=None, \n",
    "# #                                              result_format=\"BASIC\", include_config=True).to_json_dict()\n",
    "\n",
    "# # if verification_dict[\"rule_3_check\"][\"success\"]:\n",
    "# #     print(\"Rule 3 passed\")\n",
    "# # else:\n",
    "# #     print(\"Rule 3 failed\")\n",
    "# #### Rule 2,3\n",
    "# Ideally by logic cost * units should give the total cost, however there are some orders where this condition is not matching. We should confirm these condition from client.\n",
    "\n",
    "# Similar goes for Price * units\n",
    "# # # Back to Pandas\n",
    "# # orders_df = orders_df.drop('selling_cal', axis=1)\n",
    "# # orders_df = orders_df.drop('act_selling_round', axis=1)\n",
    "# # orders_df = pd.DataFrame(orders_df)\n",
    "# ### Table cardinality\n",
    "# # from  ta_lib.core.api import setanalyse\n",
    "\n",
    "# # setanalyse(orders_df.SKU.tolist(),prod_df.SKU.tolist())\n",
    "# This represents a venn diagram on two lists. Left list is `A` & right one is `B`. `A-B` implies that are five SKUs in orders_df missing in product master. We can find them using unsimplied version.\n",
    "\n",
    "# Let us look at the set `A-B`.\n",
    "# # missings_master_skus = setanalyse(orders_df.SKU.tolist(),prod_df.SKU.tolist(),simplify=False)['A-B']\n",
    "# # missings_master_skus\n",
    "# # import numpy as np\n",
    "# # print(('Records affected due to missing keys are {0} accounting to {1}% of orders').format(\n",
    "# #     orders_df.SKU.isin(missings_master_skus).sum(),np.round(orders_df.SKU.isin(missings_master_skus).mean()*100,2)))\n",
    "# **Since missing keys are very less we can proceed with inner join**\n",
    "# **Dev NOTES**\n",
    "\n",
    "# <details>\n",
    "# 1. Cardinality with mulitple keys: If you have more than one key use setanalyse_df. You can pass dataframes instead of lists and the key columns\n",
    "# 2. Excess master data (if `B-A` >0 in above example) will not be concern\n",
    "# 3. If the `A-B` is larger, please check with client for alternative data sources. In case of left join keep a stragey for imputing.\n",
    "\n",
    "# ### Master uniqueness\n",
    "\n",
    "# Product master is expected to have non duplicate primary keys. Let us verify them.\n",
    "# # # Snippet \n",
    "# # df_freq = prod_df.SKU.value_counts().reset_index()\n",
    "# # df_freq.columns = ['SKU','Frequency']\n",
    "# # fil_ = df_freq.Frequency>1\n",
    "# # if fil_.sum() > 0:\n",
    "# #     print((\"Found {0} duplicates in master. Sample duplicates are:\").format(fil_.sum()))\n",
    "# #     print(df_freq[fil_].head())\n",
    "# # else:\n",
    "# #     print(\"No duplciates in primary key\")\n",
    "# # len(prod_df)\n",
    "# # # Dropping inconsistent records\n",
    "# # print((\"No. of rows before dropping duplicate SKUs: {0}\".format(prod_df.shape[0])))\n",
    "# # fil_ = (prod_df.SKU == 'UNLKD SONY XPERIA XZS BLUE 32G') & (prod_df.color.str.strip() == 'BLACK')\n",
    "# # prod_df = prod_df[~fil_]\n",
    "# # print((\"No. of rows after dropping duplicate SKUs: {0}\".format(prod_df.shape[0])))\n",
    "# ## Health Analysis\n",
    "# Get an overview of the overall health of the dataset. This is usually quick to compute and hopefully highlights some problems to focus on.\n",
    "\n",
    "\n",
    "# ### Summary Plot\n",
    "\n",
    "# Provides a high level summary of the dataset health.\n",
    "\n",
    "# **Watch out for:**\n",
    "\n",
    "# * too few numeric values\n",
    "# * high % of missing values\n",
    "# * high % of duplicate values\n",
    "# * high % of duplicate columns \n",
    "# #health Analysis \n",
    "\n",
    "\n",
    "# sum1, plot1 = eda.get_data_health_summary(g_search, return_plot=True)\n",
    "# sum2, plot2 = eda.get_data_health_summary(product, return_plot=True)\n",
    "# sum3, plot3 = eda.get_data_health_summary(sales, return_plot=True)\n",
    "# sum4, plot4 = eda.get_data_health_summary(media, return_plot=True)\n",
    "# sum5, plot5 = eda.get_data_health_summary(themes, return_plot=True)\n",
    "# sum6, plot6 = eda.get_data_health_summary(theme_product, return_plot=True)\n",
    "\n",
    "# display_as_tabs([('g_search', plot1), ('product_manufacturer_list', plot2), ('sales_data', plot3),\n",
    "#                  ('social_media', plot4), ('theme_list', plot5), ('theme_product_list', plot6)])\n",
    "# **Dev NOTES**\n",
    "\n",
    "# <details>\n",
    "# 1. Datatypes : We have both numeric and other types. The bulk of them seem to be numeric. `Numeric` is defined to be one of [float|int|date] and the rest are categorized as `Others`. A column is assumed to have `date` values if it has the string `date` in the column name.\n",
    "\n",
    "# 2. The missing value plot seems to indicate missing values are not present but we do have them. \n",
    "\n",
    "# 3. We are looking for duplicate observations (rows in the data). The plot shows the % of rows that are an exact replica of another row (using `df.duplicated`)\n",
    "\n",
    "# 4. We are looking for duplicate features (columns in the data).\n",
    "\n",
    "# </details>\n",
    "# ### Missing Values summary\n",
    "\n",
    "# This provides an overall view focussing on amount of missing values in the dataset.\n",
    "\n",
    "# **Watch out for:**\n",
    "# * A few columns have significant number of missing values \n",
    "# * Most columns have significant number of missing values\n",
    "\n",
    "# ### Missing Values summary\n",
    "\n",
    "# sum1, plot1 = eda.get_missing_values_summary(g_search, return_plot=True)\n",
    "# sum2, plot2 = eda.get_missing_values_summary(product, return_plot=True)\n",
    "# sum3, plot3 = eda.get_missing_values_summary(sales, return_plot=True)\n",
    "# sum4, plot4 = eda.get_missing_values_summary(media, return_plot=True)\n",
    "# sum5, plot5 = eda.get_missing_values_summary(themes, return_plot=True)\n",
    "# sum6, plot6 = eda.get_missing_values_summary(theme_product, return_plot=True)\n",
    "\n",
    "# display_as_tabs([('g_search', plot1), ('product_manufacturer_list', plot2), ('sales_data', plot3),\n",
    "#                  ('social_media', plot4), ('theme_list', plot5), ('theme_product_list', plot6)])\n",
    "\t\t\t\t \n",
    "\t\t\t\t \n",
    "# **Dev notes:**\n",
    "\n",
    "# <details>\n",
    "    \n",
    "#     * By default, the following are considered missing/NA values : `[np.Nan, pd.NaT, 'NA', None]`\n",
    "#     * additional values can be passed to tigerml (add_additional_na_values)\n",
    "#     * these are applied to all columns.\n",
    "    \n",
    "#     * some of the above information can be learnt from the data discovery step (see discussion below)\n",
    "    \n",
    "# </details>\n",
    "# sum1 = eda.get_duplicate_columns(g_search)\n",
    "# sum2 = eda.get_duplicate_columns(product)\n",
    "# sum3 = eda.get_duplicate_columns(sales)\n",
    "# sum4 = eda.get_duplicate_columns(media)\n",
    "# sum5 = eda.get_duplicate_columns(themes)\n",
    "# sum6 = eda.get_duplicate_columns(theme_product)\n",
    "\n",
    "# display_as_tabs([('google_search', sum1), ('product_manufacturer_list', sum2), ('sales_data', sum3), \n",
    "#                 ('social_media', sum4), ('theme_list', sum5), ('theme_product_list', sum6)])\n",
    "\n",
    "# sum1 = eda.get_outliers(orders_df)\n",
    "# sum2 = eda.get_outliers(prod_df)\n",
    "\n",
    "# display_as_tabs([('orders', sum1), ('product', sum2)])\n",
    "# ## Health Analysis report\n",
    "\n",
    "# Generate a report that has all the above data in a single html. This could be useful to submit to a client\n",
    "# # from ta_lib.reports.api import summary_report\n",
    "\n",
    "# # summary_report(orders_df, './orders.html')\n",
    "# # summary_report(prod_df, './prod.html')\n",
    "# prod: https://drive.google.com/file/d/1TM-T5HzAYpT8_1ugM5L6Bnfxp8r3uMem/view?usp=sharing\n",
    "\n",
    "# orders: https://drive.google.com/file/d/1uvehi90v1HFtScZrtWg3pW2zpi-DFQ1Y/view?usp=sharing\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regr-py-dev-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
